{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2  #package for image display\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, hamming_loss\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#load libraries needed fOR CNN\n",
    "from vgg16 import VGG16\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing import image as image_utils\n",
    "from imagenet_utils import decode_predictions\n",
    "from imagenet_utils import preprocess_input\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read the Poster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 103 movies without poster.\n",
      "Final data dimension:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5893, 39)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "data = pd.read_csv(\"add_imdb_utf8_fixruntime_cleaned.csv\")\n",
    "data.shape\n",
    "\n",
    "# extract movies with poster information\n",
    "index = np.zeros(data.shape[0])\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    try:\n",
    "        np.isnan(data.iloc[i, 0])\n",
    "    except:\n",
    "        index[i] = 1\n",
    "        \n",
    "print (\"There are {} movies without poster.\".format(np.sum(index == 0)))\n",
    "\n",
    "# extract data with poster\n",
    "data_with_poster = data.iloc[index == 1, ]\n",
    "print (\"Final data dimension:\")\n",
    "data_with_poster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Final data shape:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5893, 180, 128, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### read each image and resize \n",
    "\n",
    "# total number of posters\n",
    "n = data_with_poster.shape[0]\n",
    "\n",
    "# target image size\n",
    "target_length = 180\n",
    "target_width = 128\n",
    "\n",
    "# np.array to store the whole matrix in the order of the movie id\n",
    "img_matrix = np.zeros([n, target_length, target_width, 3])\n",
    "\n",
    "for i in range(n):\n",
    "    if i % 1000 == 0:    \n",
    "        print (i)    \n",
    "    \n",
    "    movie_id = data_with_poster[\"id\"].values[i]\n",
    "    \n",
    "    path = 'posters_2011_2016/' + str(movie_id) + '.jpg'\n",
    "    \n",
    "    # read image\n",
    "    img = cv2.imread(path)\n",
    "    \n",
    "    # normalize the image[-1,1], center at 0\n",
    "    # so that the zero padding does not affect the image\n",
    "    img = img.astype('float32')\n",
    "    img = (img - 255/2)/255\n",
    "    \n",
    "    # extract the length and width of the image\n",
    "    img_length, img_width = np.asarray(img.shape[:2])\n",
    "\n",
    "    \n",
    "    ## put the image to the center of the array\n",
    "    # calcuate location of the left edge\n",
    "    left_loc = int(0.5*(target_width - img_width))\n",
    "    # calcuate location of the top edge\n",
    "    top_loc = int(0.5*(target_length - img_length))\n",
    "    \n",
    "    # store the image\n",
    "    img_matrix[i, top_loc:(top_loc+img_length), \n",
    "               left_loc:(left_loc+img_width), :] = img\n",
    "    \n",
    "print (\"Final data shape:\")\n",
    "img_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (5893, 180, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# reformat the data shape\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    img_matrix = img_matrix.reshape(n, 3, target_length, target_width)\n",
    "    input_shape = (3, target_length, target_width)\n",
    "else:\n",
    "    img_matrix = img_matrix.reshape(n, target_length, target_width, 3)\n",
    "    input_shape = (target_length, target_width, 3)\n",
    "\n",
    "print('data shape:', img_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### extract and clean the response variable \n",
    "# convert genre ids to list\n",
    "y = list()\n",
    "for i in range(data_with_poster.shape[0]):\n",
    "    genre = map(int, re.sub(\"[\\[ \\] ]\", \"\", data['genre_ids'][i]).split(','))\n",
    "    y.append(genre)\n",
    "\n",
    "# binarize response variable (returns array)\n",
    "y_binary = MultiLabelBinarizer().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### split the train and test data\n",
    "# shuffle the samples\n",
    "seed = 109\n",
    "index_2 = range(n)\n",
    "shuffle(index_2)\n",
    "\n",
    "img_matrix = img_matrix[index_2]\n",
    "y_binary = y_binary[index_2]\n",
    "\n",
    "# use the training data to fit the model\n",
    "split_ratio = 0.9\n",
    "split_num = int(n * split_ratio)\n",
    "\n",
    "x_train = img_matrix[:split_num]\n",
    "x_test = img_matrix[split_num:]\n",
    "\n",
    "y_train = y_binary[:split_num]\n",
    "y_test = y_binary[split_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047457627118644069"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # check how many animations in the test data\n",
    "# np.mean(y_test[:, 2] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subsample the training data for more balanced class (19:1 to 2:1)\n",
    "size = 2 * np.sum(y_train[:, 2] == 1)\n",
    "\n",
    "# extract movies with animation label\n",
    "x_animation = x_train[y_train[:, 2] == 1, ]\n",
    "y_animation = y_train[y_train[:, 2] == 1, ]\n",
    "\n",
    "# sample from the rest of data\n",
    "x_no_animation = x_train[y_train[:, 2] != 1, ]\n",
    "y_no_animation = y_train[y_train[:, 2] != 1, ]\n",
    "\n",
    "index = range(x_no_animation.shape[0])\n",
    "shuffle(index)\n",
    "\n",
    "x_no_subsample = x_no_animation[index[1:size], ]\n",
    "y_no_subsample = y_no_animation[index[1:size], ]\n",
    "\n",
    "# mix the data and shuffle\n",
    "x_subsample = np.concatenate([x_animation, x_no_subsample], axis = 0)\n",
    "y_subsample = np.concatenate([y_animation, y_no_subsample], axis = 0)\n",
    "\n",
    "index = range(x_subsample.shape[0])\n",
    "shuffle(index)\n",
    "\n",
    "x_subsample = x_subsample[index]\n",
    "y_subsample = y_subsample[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import VGG16 and overfit on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load pretrained model - VGG16\n",
    "base_model =VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "x = base_model.output \n",
    "x = GlobalAveragePooling2D()(x) # let's add a fully-connected layer \n",
    "#x = Dropout(0.3)(x)\n",
    "x = Dense(1024, activation='relu')(x) # and a logistic layer -- let's say we have 200 classes \n",
    "predictions = Dense(1, activation='sigmoid')(x)  # this is the model we will train \n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# set the first 15 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:12]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=0.001, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# specify the tuning parameters\n",
    "# smaller batch size means noisier gradient, but more updates per epoch\n",
    "batch_size = 20\n",
    "# number of iterations over the complete training data\n",
    "epochs = 50\n",
    "# add early stopping\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5303, 180, 128, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_subsample, y_subsample[:, 2],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1)\n",
    "                    #callbacks=[earlyStopping],\n",
    "                    #validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.save('finetune_base_overfit_animation.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fine tune the final layers using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "base_model = load_model('finetune_base_overfit_animation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# add dropouts and the dense layers\n",
    "x = base_model.layers[19].output \n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x)  # this is the model we will train \n",
    "model2 = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set the first 19 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model2.layers[:20]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# smaller batch size means noisier gradient, but more updates per epoch\n",
    "batch_size = 20\n",
    "# number of iterations over the complete training data\n",
    "epochs = 50\n",
    "# add early stopping\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=0.001, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "history = model2.fit(x_subsample, y_subsample[:, 2],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1, \n",
    "                    #callbacks=[earlyStopping],\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = y_pred.reshape(len(y_pred))\n",
    "\n",
    "y_bin = np.zeros(len(y_pred))\n",
    "y_bin[y_pred > 0.5] = 1\n",
    "\n",
    "y_actual = y_test[:, 2].reshape(len(y_pred))\n",
    "\n",
    "test_result = pd.DataFrame({\n",
    "    \"prediction\": y_bin,\n",
    "    \"actual\": y_actual,\n",
    "    \"probability\": y_pred\n",
    "})\n",
    "\n",
    "test_result.to_csv(\"prediction_animation.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.114285714286\n",
      "Accuracy:  0.684745762712\n",
      "Precision:  0.0722891566265\n",
      "Recall:  0.272727272727\n",
      "Hamming loss: 0.315254237288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[392, 154],\n",
       "       [ 32,  12]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the above code was run on AWS, and test prediction was saved\n",
    "# read the prediction and compute confusion matrix\n",
    "result = pd.read_csv(\"prediction_animation.csv\")\n",
    "\n",
    "print \"F1 score: \", f1_score(result[\"actual\"], result[\"prediction\"])\n",
    "print \"Accuracy: \", np.mean(result[\"actual\"] == result[\"prediction\"])\n",
    "print \"Precision: \", precision_score(result[\"actual\"], result[\"prediction\"])\n",
    "print \"Recall: \", recall_score(result[\"actual\"], result[\"prediction\"])\n",
    "print \"Hamming loss:\", hamming_loss(result[\"actual\"], result[\"prediction\"])\n",
    "confusion_matrix(result[\"actual\"], result[\"prediction\"])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
