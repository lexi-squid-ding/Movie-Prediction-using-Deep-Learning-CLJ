{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3: Traditional statistical and machine learning methods\n",
    "\n",
    "By: Alexandra Ding, Cynthia He, Jingyi Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "Before modeling, we constructed a sample data set of about 6000 movies and 124 predictors. The predictors include movie features TMDB and IMDB and popular word counts from movie titles and movie reviews.\n",
    "\n",
    "The predictors are:\n",
    " - Release year (TMDB, factor of 6 levels)\n",
    " - Release month (TMDB, factor of 12 levels)\n",
    " - Vote count (TMDB)\n",
    " - Popularity (TMDB)\n",
    " - Vote average (TMDB)\n",
    " - Runtime (IMDB)\n",
    " - Aspect ratio (IMDB, factor of 22 levels)\n",
    " - Count of keywords in movie title (TMDB, 17 words with more than 30 total occurrence were selected )\n",
    " - Count of keywords in movie overview (TMDB, 66 words with more than 200 total occurrence were selected) \n",
    "\n",
    "Response variable:\n",
    " - Movie genre: binarized (One vs Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape: (5996, 124)\n",
      "X_data_std shape: (5996, 124)\n",
      "y shape (5996, 20)\n"
     ]
    }
   ],
   "source": [
    "### Load Dataset\n",
    "# X: Unprocessed features\n",
    "# X_std: standardized by Preprocessor\n",
    "# y: MultiLabel Binarized targets\n",
    "[X_data, X_data_std, y_data] = pickle.load(open('continuous_features_targets.p', 'rb'))\n",
    "\n",
    "print 'X_data shape:', X_data.shape\n",
    "print 'X_data_std shape:', X_data_std.shape\n",
    "print 'y shape', y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Model Description\n",
    "\n",
    "Because a single movie can be associated with several different labels, this represents a **multi-label classification** problem where we have to adapt binary classifiers to the multi-label scenario. Methods to adapt classifiers such as Logistic Regression, Decision Trees, Random Forest and Support Vector Machines to the multilabel case include fitting a model on a single label at a time (**One vs Rest Classifier**) and then assigning multiple predicted labels to data points that are predicted to belong to the labeled cases. Other methods to perform multilabel classification include one-vs-one classification (where each pair of labels is compared) and RAKEL, where a random subset of labels is trained against the remaining labels (SOURCE) \n",
    "\n",
    "Because One vs Rest classification is computationally efficient (requiring only n_classes classifiers) and highly interpretable, we decided to use this way to transform our multilabel classification problem into a set of binary classifications. Python’s Sklearn Machine Learning package can generalize many classification models to the One vs Rest case (OneVsRestClassifier in sklearn.multiclass). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models fitted on the data\n",
    "\n",
    "We selected two models to predict the binarized response variable using the movie features. The models were selected according to their computational efficiency and performance. The relevant parameters were tuned using 3-fold cross validation. \n",
    "\n",
    " - Weighted Logistic regression: Tuned **Regularization Parameter C** using Cross-Validation.  \n",
    " - Decision Tree: Tuned **max_depth** using cross validation\n",
    " - Random Forest: Tuned **max_depth** using cross validation\n",
    " - AdaBoost, Gradient Boosting \n",
    "**NOTE: We ran the models on separate computers, rather than in a single notebook. The results were saved in a pickle file for each and are analyzed below**\n",
    "\n",
    "We also tried but did not finish running: \n",
    "- SVM (Linear, Polynomial, SVM)- these took a long time (i.e. more than several hours) to run on computers or on AWS, so we excluded these from our report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Cross Validation and Model Selection metrics\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss \n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Preprocessing\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler as Standardize\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "### Classification\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as Log_Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load Dataset\n",
    "# X: Unprocessed features\n",
    "# X_std: standardized by Preprocessor\n",
    "# y: MultiLabel Binarized targets\n",
    "[X_data, X_data_std, y_data] = pickle.load(open('continuous_features_targets.p', 'rb'))\n",
    "\n",
    "######      CREATE SCORING METRIC        ######\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html\n",
    "\n",
    "# Hamming Loss: want to MINIMIZE LOSS \n",
    "hamming_scorer = make_scorer(hamming_loss, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "The first model we tried is regularized **Logistic Regression** with **balanced class weights**. \n",
    "\n",
    "Logistic regression is the most commonly used statistical model for binary classification. It is a generalized linear model which utilizes a logit link to associate the linear combination of the predictors and the class probability (i.e. $log(\\frac{p}{1-p}) = X’\\beta$). In order to overcome the imbalanced nature of the data, we used balanced weight when fitting the logistic regression.\n",
    "\n",
    "Logistic regression is a parametric model, and there are two important underlying assumption of this model: 1) each observations are independent; 2) the relationship between the predictors and logit class probability (i.e. $log(\\frac{p}{1-p})$). Although the data sample may satisfy the first assumption, the second assumption is very likely to be violated. This is probably an important reason why that the logistic regression model does not perform well, so we moved on to try several non-parametric models: decision tree, random forest and adaboost. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Logistic Regression\n",
    "LogReg_Model = OneVsRestClassifier(Log_Reg(penalty = 'l2', class_weight = 'balanced'))\n",
    "\n",
    "LogReg_grid = GridSearchCV(LogReg_Model, \n",
    "                           param_grid={'estimator__C': np.logspace(-5, 15, 20)}, \n",
    "                                       scoring= hamming_scorer,\n",
    "                                       n_jobs = 5)\n",
    "LogReg_grid.fit(X_data_std, y_data)\n",
    "LogReg_grid.cv_results_['mean_test_score']\n",
    "\n",
    "# Fit best model on data, predict\n",
    "y_pred_LogReg = cross_val_predict(LogReg_grid.best_estimator_, X_data_std, y_data)\n",
    "\n",
    "# Dump CV results AND predictions from best model\n",
    "pickle.dump([LogReg_grid.cv_results_, y_pred_LogReg], open('LogReg_grid_results.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Next, we tried to use a single Decision Tree. Some of the advantages of the Decision Tree include interpretability (the splitting criterion for each branch is easy to interpret), scale invariance, ability to handle both categorical and quantitative variables (this is beneficial for our dataset due to the presence of variables such as runtime and season) and can approximate a wide variety of distributions of data. The reason why we started with a Decision Tree is to gain intuition into whether our multilabel problem is easily solved by tree-based methods. \n",
    "\n",
    "Here, we tuned the parameter **max_depth**, which controls how far the tree expands and, indirectly, the number of samples in each node. Restricting the parameter max_depth can reduce overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Single Decision Tree\n",
    "DecisionTree_Model = OneVsRestClassifier(tree.DecisionTreeClassifier(criterion='gini'))\n",
    "DT_grid = GridSearchCV(DecisionTree_Model, \n",
    "                    param_grid = {'estimator__max_depth': range(1,10)},\n",
    "                                  scoring = hamming_scorer)\n",
    "DT_grid.fit(X_data_std, y_data)\n",
    "DT_grid.cv_results_['mean_test_score']\n",
    "print 'Best score single DT:', DT_grid.best_score_\n",
    "\n",
    "y_pred_Decision_Tree = cross_val_predict(DT_grid.best_estimator_, X_data_std, y_data)\n",
    "np.mean(y_pred_Decision_Tree == y_data)\n",
    "\n",
    "pickle.dump([DT_grid.cv_results_, y_pred_Decision_Tree], open('DecisionTree_grid_results.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "A Random Forest consists of many decision trees, and each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features (SOURCE: sklearn documentation http://scikit-learn.org/stable/modules/ensemble.html#forest). \n",
    "\n",
    "In the random forest, parameters we could tune include the number of trees, the maximum number of features, the maximum depth, and minimum number of samples at the leaf node. As we saw in lecture, the test classification error decreases with the number of trees and eventually levels off. Note that Random Forests **cannot overfit the data with respect to the number of trees**, because adding trees does not increase the flexibiltiy of the model. The maximum number of features, as previously discussed, allows us to tune the chance that relevant features will be selected, and thus influences misclassification error. Together, the max depth and min samples at leaf node influence how \"full-grown\" a tree is, meaning how many splits are made in the algorithm. \n",
    "\n",
    "Given these parameters, I decided to adjust **max_depth**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Random Forest\n",
    "RandomForest_Model = OneVsRestClassifier(RandomForest())\n",
    "rf_grid = GridSearchCV(RandomForest_Model,\n",
    "                       param_grid = {'estimator__max_depth': 10*np.linspace(1,7, 7) },\n",
    "scoring = hamming_scorer)\n",
    "\n",
    "rf_grid.fit(X_data_std, y_data)\n",
    "rf_grid.cv_results_['mean_test_score']\n",
    "print 'Best score in RF tuning max_depth:', rf_grid.best_score_\n",
    "y_pred_RF = cross_val_predict(rf_grid.best_estimator_, X_data_std, y_data)\n",
    "\n",
    "pickle.dump([rf_grid.cv_results_, y_pred_RF], open('RandomForest_tune_maxdepth_hamming_grid_results.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "In AdaBoost, we fit \"weak learners\" (such as small decision trees/stumps) to the data, and then use an iterative algorithm to \"adapt\" to the misclassified samples in order to boost the classification accuracy. Initially, all samples are equally weighted, and with each iteration, the weights are adjusted based on whether the last classification misclassified the sample. (SOURCE: http://scikit-learn.org/stable/modules/ensemble.html#forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Adaboost\n",
    "\n",
    "clf = OneVsRestClassifier(AdaBoostClassifier())\n",
    "ada_grid = GridSearchCV(clf,\n",
    "                       param_grid = {'estimator__n_estimators': np.logspace(1,3,6).astype(int) },\n",
    "                                     scoring = hamming_scorer)\n",
    "#ada_grid.fit(iris.data, iris.target)\n",
    "ada_grid.fit(X_data_std, y_data)\n",
    "ada_grid.cv_results_['mean_test_score']\n",
    "print 'Best score in RF tuning max_depth:', ada_grid.best_score_\n",
    "\n",
    "#scores = cross_val_score(ada_grid.best_estimator_, X_data_std, y_data, scoring = hamming_scorer)\n",
    "y_pred_ada = cross_val_predict(ada_grid.best_estimator_, X_data_std, y_data)\n",
    "pickle.dump([ada_grid.cv_results_, y_pred_ada], open('Adaboost_grid_results.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Model Performance for Multilabel Classification\n",
    "\n",
    "### Describe performance metrics\n",
    "\n",
    "To perform multilabel classification using the OneVsAll classifier, we used several different metrics to assess model performance. Six different metrics were used as the performance metrics of our model: 0-1 loss based accuracy, Hamming loss, precision, recall, F1-score, and Jaccard Similarity. In addition to reporting these metrics for the overall dataset under different models, we evaluated each of these metrics for each label, to detect whether any labels were being excluded from prediction due to imbalance. \n",
    "\n",
    "\n",
    "#### 1. Accuracy\n",
    "\n",
    "Accuracy is calculated on a zero-one loss basis, which counts a match between the prediction and true value as one. It then takes the mean of all the indicator and produced the percentage that the binary entries in prediction matrix correctly match up with true values.\n",
    "\n",
    "#### 2. Hamming Loss\n",
    "\n",
    "Simply put, the Hamming loss is the fraction of labels that are incorrectly predicted, penalized according to sample weights. While the zero-one loss considers the entire set of labels for a given sample incorrect if it doesn’t entirely match the true set of labels, the Hamming loss is more forgiving in that it penalizes the individual labels. One benefit of using this loss function (cited in this review) is that different costs can be specified for different types of errors (ex: false positives and true negatives). \n",
    "\n",
    "$$Accuracy(H,D) = \\frac{1}{|D|} \\sum_{i=1}^{|D|} ( \\frac{|Y_i \\cap Z_i|}{|Y_i \\cup Z_i|}^a$$\n",
    "\n",
    "Where: D = dataset, Y_i = dataset labels, H = multilabel classification output, Z_i = set of labels predicted by H for example X_i. The parameter a is the “forgiveness rate” that calibrates the severity of different errors. This can be adjusted by the “sample_weight” argument in sklearn. \n",
    "\n",
    "**We used Hamming Loss as the scoring parameter used to tune our model parameters via Cross-Validation.** In this case, we used the default (uniform) forgiveness rate, but future calibration of our models may require that we weight the samples to offset the label imbalance in our dataset. \n",
    "\n",
    "Because **a larger Hamming Loss denotes a worse prediction accuracy,** we had to multiply this value by -1 to use it in the GridSearchCV function below. \n",
    "\n",
    "#### 3. Precision\n",
    "\n",
    "$$Precision=\\frac{tp}{tp+fp}$$\n",
    "where tp = True Positives and fp = False Positives\n",
    "\n",
    "True positives are the correctly identified positive instances, and false positives are the negative instances which were incorrectly identified as positives. The sum of true positives and false positives is the total number of positive instances identified in the prediction. Thus, the Precision is the proportion of correctly identified positive instances over the total number of predicted positive instances. \n",
    "\n",
    "A low Precision can occur if many movies are being labeled with a given genre but few of them actually belong to that genre. \n",
    "\n",
    "#### 4. Recall\n",
    "\n",
    "$$Recall=\\frac{tp}{tp+fn}$$ \n",
    "where tp = True Positives and fn = False Negatives\n",
    "\n",
    "Recall calculates very similarly as the prediction but uses a different numerator. The false negatives are the positive instances which were incorrectly identified as negative. True positives and false negatives sum up to the total number of true positive instances in the data. Therefore, the Recall is the proportion of correctly identified positive instances over the total number of true positive instances. \n",
    "\n",
    "A low Recall score would suggest that few movies are correctly labeled in predictions, in proportion with the number of movies truly with that label.\n",
    "\n",
    "#### 5. F-1 Score\n",
    "\n",
    "F-1 score is the harmonic mean of Precision and Recall. It is a measurement that considers both Precision and Recall. \n",
    "\n",
    "$$F_{1}=2\\cdot {\\frac {1}{{\\tfrac {1}{\\mathrm {recall} }}+{\\tfrac {1}{\\mathrm {precision} }}}}=2\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\mathrm {precision} +\\mathrm {recall} }}$$\n",
    "\n",
    "#### 6. Jaccard Similarities\n",
    "\n",
    "The Jaccard Similarity between the predicted labels (y_pred) and ground truth labels (y_data) is defined as the intersection divided by the size of the union of the two label sets for a given data point $X_i$. Therefore, JS penalizes the inclusion or exclusion of true labels from the prediction. \n",
    "\n",
    "$$ J(A,B) = {{|A \\cap B|}\\over{|A \\cup B|}} = {{|A \\cap B|}\\over{|A| + |B| - |A \\cap B|}}$$\n",
    "where A = dataset labels, B = multilabel classification output\n",
    "\n",
    "Like the Hamming Loss, the weights of different samples can be changed based on label imbalance, though we used default values when assessing our classifiers below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import hamming_loss \n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import jaccard_similarity_score as jaccard_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "os.chdir('/Users/AlexandraDing/Documents/cs109b-best-group')\n",
    "\n",
    "# WD Where the model results are kept\n",
    "data_wd = '/Users/AlexandraDing/Documents/cs109b-best-group/Model_Results/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data shape: (5996, 124)\n",
      "X_data_std shape: (5996, 124)\n",
      "y shape (5996, 20)\n",
      "(5996, 20)\n",
      "(5996, 20)\n",
      "(5996, 20)\n",
      "(5996, 20)\n"
     ]
    }
   ],
   "source": [
    "### Load Dataset\n",
    "# X: Unprocessed features\n",
    "# X_std: standardized by Preprocessor\n",
    "# y: MultiLabel Binarized targets\n",
    "[X_data, X_data_std, y_data] = pickle.load(open('continuous_features_targets.p', 'rb'))\n",
    "\n",
    "print 'X_data shape:', X_data.shape\n",
    "print 'X_data_std shape:', X_data_std.shape\n",
    "print 'y shape', y_data.shape\n",
    "\n",
    "# Unpickle Model Results\n",
    "[LogReg_cv, y_pred_LogReg] = pickle.load(open(data_wd+'LogReg_grid_results.p', 'rb'))\n",
    "print y_pred_LogReg.shape\n",
    "\n",
    "[DT_grid_cv, y_pred_Decision_Tree] = pickle.load(open(data_wd + 'DecisionTree_grid_results.p', 'rb'))\n",
    "print y_pred_Decision_Tree.shape\n",
    "\n",
    "[rf_grid_cv, y_pred_RF] = pickle.load(open(data_wd + 'RandomForest_tune_maxdepth_hamming_grid_results.p', 'rb'))\n",
    "print y_pred_RF.shape\n",
    "\n",
    "[ada_grid_cv, y_pred_ada] = pickle.load(open(data_wd + 'Adaboost_grid_results.p', 'rb'))\n",
    "print y_pred_ada.shape\n",
    "\n",
    "# Create list with all model prediction\n",
    "prediction_list = [y_pred_LogReg, y_pred_Decision_Tree, y_pred_RF, y_pred_ada]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adventure           367\n",
      "Fantasy             268\n",
      "Animation           339\n",
      "Drama              2179\n",
      "Horror              856\n",
      "Action              774\n",
      "Comedy             1496\n",
      "History             125\n",
      "Western              55\n",
      "Thriller           1157\n",
      "Crime               396\n",
      "Documentary         909\n",
      "Science Fiction     422\n",
      "Mystery             269\n",
      "Music               286\n",
      "Romance             650\n",
      "Family              460\n",
      "War                  73\n",
      "TV Movie            227\n",
      "dtype: int64\n",
      "[u'Adventure', u'Fantasy', u'Animation', u'Drama', u'Horror', u'Action', u'Comedy', u'History', u'Western', u'Thriller', u'Crime', u'Documentary', u'Science Fiction', u'Mystery', u'Music', u'Romance', u'Family', 'Foreign', u'War', u'TV Movie']\n"
     ]
    }
   ],
   "source": [
    "# Read column names to get genre labels for tables below\n",
    "movie_data = pd.read_csv(\"add_imdb_utf8_fixruntime_cleaned.csv\")\n",
    "# print movie_data.head(n=3)\n",
    "genre_numbers = movie_data.columns[14:33]\n",
    "genre_dict = pickle.load(open('/Users/AlexandraDing/Documents/cs109b-best-group/Milestone1/genre_dict_by_id.p', 'rb'))\n",
    "genre_labels = [genre_dict[int(genre_numbers[i])] for i in range(len(genre_numbers))]\n",
    "print pd.Series(data= np.sum(movie_data[genre_numbers], axis = 0).values, index=genre_labels)\n",
    "genre_labels.insert(17, \"Foreign\")\n",
    "print genre_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### SUMMARIZE MODEL ACCURACY: \n",
    "    # for MULTILABEL DATA, calculates baseline accuracy, hamming loss, f1 score, jaccard similarity, classification report\n",
    "    # INPUTS:\n",
    "        # y_prediction: predicted y\n",
    "        # y_data : ground truth y\n",
    "    # OUTPUTS:\n",
    "        # prints accuracy metrics\n",
    "        # Return 0\n",
    "\n",
    "def summarize_model_accuracy (y_prediction, y_data, names):\n",
    "    # Get basic accuracy: what proportion of labels are correct\n",
    "    print 'Accuracy:', np.mean(y_prediction == y_data)\n",
    "    \n",
    "    # Get Hamming Loss\n",
    "    print 'Hamming Loss:', hamming_loss(y_data, y_prediction)\n",
    "    \n",
    "    # Get f1\n",
    "    print 'F1 Score:', f1_score(y_data, y_prediction, average = 'weighted')\n",
    "    \n",
    "    # get Jaccard Similarity\n",
    "    print 'Jaccard Similarity:', jaccard_score(y_data, y_prediction)\n",
    "    \n",
    "    # Classification report:report recall, precision, f1 ON EACH CLASS (can be used for multilabel case)\n",
    "    print classification_report(y_data, y_prediction, target_names = names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.595488659106\n",
      "Hamming Loss: 0.404511340894\n",
      "F1 Score: 0.363745775457\n",
      "Jaccard Similarity: 0.151680275982\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.10      0.61      0.18       367\n",
      "        Fantasy       0.07      0.66      0.13       268\n",
      "      Animation       0.10      0.80      0.18       339\n",
      "          Drama       0.52      0.67      0.58      2179\n",
      "         Horror       0.22      0.74      0.35       856\n",
      "         Action       0.20      0.74      0.32       774\n",
      "         Comedy       0.36      0.68      0.47      1496\n",
      "        History       0.04      0.80      0.07       125\n",
      "        Western       0.02      0.73      0.03        55\n",
      "       Thriller       0.29      0.76      0.42      1157\n",
      "          Crime       0.09      0.71      0.17       396\n",
      "    Documentary       0.30      0.91      0.45       909\n",
      "Science Fiction       0.11      0.77      0.20       422\n",
      "        Mystery       0.06      0.67      0.11       269\n",
      "          Music       0.08      0.73      0.15       286\n",
      "        Romance       0.20      0.67      0.30       650\n",
      "         Family       0.15      0.69      0.24       460\n",
      "        Foreign       0.02      0.79      0.04        73\n",
      "            War       0.01      0.47      0.02        74\n",
      "       TV Movie       0.07      0.64      0.12       227\n",
      "\n",
      "    avg / total       0.27      0.72      0.36     11382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summarize LogReg Performance\n",
    "LogRegSummary = summarize_model_accuracy(y_pred_LogReg, y_data, genre_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.908522348232\n",
      "Hamming Loss: 0.0914776517678\n",
      "F1 Score: 0.168331087028\n",
      "Jaccard Similarity: 0.13245377871\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.59      0.04      0.08       367\n",
      "        Fantasy       0.14      0.01      0.01       268\n",
      "      Animation       0.45      0.05      0.09       339\n",
      "          Drama       0.56      0.27      0.37      2179\n",
      "         Horror       0.00      0.00      0.00       856\n",
      "         Action       0.49      0.03      0.06       774\n",
      "         Comedy       0.86      0.12      0.22      1496\n",
      "        History       0.08      0.01      0.01       125\n",
      "        Western       0.00      0.00      0.00        55\n",
      "       Thriller       0.25      0.00      0.00      1157\n",
      "          Crime       0.00      0.00      0.00       396\n",
      "    Documentary       0.82      0.30      0.44       909\n",
      "Science Fiction       0.30      0.03      0.06       422\n",
      "        Mystery       0.00      0.00      0.00       269\n",
      "          Music       0.65      0.15      0.24       286\n",
      "        Romance       0.42      0.11      0.18       650\n",
      "         Family       0.61      0.04      0.08       460\n",
      "        Foreign       0.29      0.05      0.09        73\n",
      "            War       0.00      0.00      0.00        74\n",
      "       TV Movie       0.34      0.07      0.12       227\n",
      "\n",
      "    avg / total       0.47      0.11      0.17     11382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Decision_Tree = summarize_model_accuracy(y_pred_Decision_Tree, y_data, genre_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.909372915277\n",
      "Hamming Loss: 0.0906270847231\n",
      "F1 Score: 0.215170777762\n",
      "Jaccard Similarity: 0.169916055148\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.52      0.06      0.11       367\n",
      "        Fantasy       0.27      0.01      0.02       268\n",
      "      Animation       0.52      0.04      0.07       339\n",
      "          Drama       0.60      0.34      0.43      2179\n",
      "         Horror       0.62      0.09      0.16       856\n",
      "         Action       0.44      0.05      0.09       774\n",
      "         Comedy       0.64      0.17      0.27      1496\n",
      "        History       0.00      0.00      0.00       125\n",
      "        Western       0.00      0.00      0.00        55\n",
      "       Thriller       0.40      0.10      0.16      1157\n",
      "          Crime       0.14      0.00      0.00       396\n",
      "    Documentary       0.76      0.35      0.48       909\n",
      "Science Fiction       0.47      0.02      0.04       422\n",
      "        Mystery       0.00      0.00      0.00       269\n",
      "          Music       0.63      0.08      0.15       286\n",
      "        Romance       0.48      0.07      0.12       650\n",
      "         Family       0.52      0.06      0.11       460\n",
      "        Foreign       0.00      0.00      0.00        73\n",
      "            War       0.00      0.00      0.00        74\n",
      "       TV Movie       0.33      0.03      0.06       227\n",
      "\n",
      "    avg / total       0.51      0.15      0.22     11382\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "RF_Model_Summary = summarize_model_accuracy(y_pred_RF, y_data, genre_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.909856571047\n",
      "Hamming Loss: 0.0901434289526\n",
      "F1 Score: 0.265702827649\n",
      "Jaccard Similarity: 0.21204890562\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Adventure       0.49      0.10      0.17       367\n",
      "        Fantasy       0.00      0.00      0.00       268\n",
      "      Animation       0.43      0.09      0.15       339\n",
      "          Drama       0.58      0.39      0.47      2179\n",
      "         Horror       0.62      0.19      0.29       856\n",
      "         Action       0.47      0.09      0.15       774\n",
      "         Comedy       0.70      0.18      0.29      1496\n",
      "        History       0.26      0.05      0.08       125\n",
      "        Western       0.00      0.00      0.00        55\n",
      "       Thriller       0.46      0.15      0.23      1157\n",
      "          Crime       0.00      0.00      0.00       396\n",
      "    Documentary       0.71      0.45      0.55       909\n",
      "Science Fiction       0.43      0.02      0.04       422\n",
      "        Mystery       0.00      0.00      0.00       269\n",
      "          Music       0.62      0.13      0.22       286\n",
      "        Romance       0.52      0.13      0.21       650\n",
      "         Family       0.40      0.08      0.13       460\n",
      "        Foreign       0.20      0.04      0.07        73\n",
      "            War       0.00      0.00      0.00        74\n",
      "       TV Movie       0.45      0.11      0.18       227\n",
      "\n",
      "    avg / total       0.50      0.19      0.27     11382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ada_Boost = summarize_model_accuracy(y_pred_ada, y_data, genre_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison and Results\n",
    "\n",
    " - Logistic regression\n",
    "The strength of logistic regression is that the model is computationally fast, and the results can be well interpreted, as long as the assumptions are met. The performance of the model can also be assessed by plotting the residuals and Cook’s distance. It can be regularized to prevent overfitting. However, logistic regression usually perform well on data that are linearly separable. It is less common to apply kernels to the model, and correctly selecting higher degrees of the predictors is hard.\n",
    "\n",
    " - Decision tree\n",
    "The advantage of decision trees is that the models can be initiatively explained. It is easy to see how the decision is made at every step. (It has been discussed above.) However, the model performance is not stable.\n",
    "\n",
    " - Random Forest and Ada Boost:\n",
    "Random forest  and Ada Boost are advanced versions of decision trees, and they perform much more stably because the ensemble of multiple decision trees. However, they loose the interpretability of the model. \n",
    " \n",
    "According to the model accuracy measurements above, Ada boost is currently our best performing model. Its performance  is consistent better than the others across multiple performance metrics: having the highest 0-1 loss based accuracy,  precision, F-1 score and Jaccard Similarity and having the lowest Hamming loss.\n",
    "\n",
    "As mentioned in the model comparison that logistic regression model is parametric, which is not as flexible as decision trees. Random forest and Ada boost are ensembled version of decision trees. Reasonably, ada boost out perform the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Challenges and Next Steps\n",
    "\n",
    "We encountered various challenges in the model implementation process:\n",
    "\n",
    " - **High commuptation intensity:**\n",
    "\n",
    "    SVM is very computational intensive. We tried to run it on AWS x2.large instance, but fitting the SVM models still requires several hours. We are working on establishing parallel computing, and would include SVM models if we have more time. \n",
    "    \n",
    " \n",
    " - **High dimensionality:**\n",
    " \n",
    "    Text analysis on the movie title and overview provided us over 10 thousand words. We tried to reducing the dimensionality through PCA, but over 3000 principal components are required to explain 90% of the variability. Therefore, we decided to set reasonable thresholds on the total occurrence of the words, and only use the words with ocurrence that is beyond the threshold. In this way, we are able to reduce the number of predictors and keep the interpretbility, although some useful information may be lost. \n",
    " \n",
    "At the moment, we are running SVM with different kernels, and we are looking forward to seeing the results. Regarding the next step, we plan to try Gradient Boosting and fine tune the parameters in the realm of traditional statistical and machine learning methods. At the sametime, we would like to explore the deep learning method and utilize image information coming from movie posters. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
